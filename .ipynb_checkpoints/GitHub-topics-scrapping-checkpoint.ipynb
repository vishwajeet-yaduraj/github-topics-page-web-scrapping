{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6edab64-c05e-4d24-af6a-7b604c1370b3",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce367a-7882-4adb-a19d-b95950ad4f90",
   "metadata": {},
   "source": [
    "## Introduction about web scraping\n",
    "Web scraping is a technique used to automatically extract information from websites. It involves making HTTP requests to a website, retrieving the HTML content, and then parsing that content to extract the desired data. Web scraping is particularly useful when the information you need is spread across multiple web pages and not available through an official API. By automating the process of data collection, web scraping can save time and effort, allowing you to gather large datasets for analysis, research, or personal use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293bf76-5cee-4430-8b41-d2d499ecf773",
   "metadata": {},
   "source": [
    "## Introduction about GitHub\n",
    "GitHub is one of the most widely used platforms for version control and collaboration, primarily among software developers. It allows users to host and share code repositories, contribute to open-source projects, and collaborate on software development. Given the vast amount of data hosted on GitHub, it can be challenging to keep track of trending repositories or find repositories on specific topics of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe26a2f-28b5-4caa-ab10-65c0afe48e2d",
   "metadata": {},
   "source": [
    "##  The problem statement\n",
    "The problem we're addressing in this project is how to efficiently identify and analyze top repositories on GitHub based on specific topics. Scraping GitHub's topics pages allows us to gather a curated list of repositories that are categorized by various themes such as machine learning, web development, or data science. By analyzing these repositories, we can gain insights into the most popular projects, observe trends in technology, and even explore potential projects for personal learning or contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0971f-6e24-4285-aca4-597d5bbea90f",
   "metadata": {},
   "source": [
    "## Tools Mentioned\n",
    "In this project, we’ll be using Python along with a few powerful libraries:\n",
    "\n",
    "* requests: This library is used to send HTTP requests to websites and retrieve their content. It is simple to use and allows us to interact with web pages as if we were browsing them in a web browser.\n",
    "\n",
    "* BeautifulSoup: A library for parsing HTML and XML documents. It helps us navigate through the HTML content of a web page and extract the specific data we need. BeautifulSoup is particularly useful for web scraping because it simplifies the process of locating elements on a page by their tags, attributes, or hierarchy.\n",
    "\n",
    "* pandas: A data manipulation library that allows us to organize, analyze, and manipulate the data we scrape. Once we have extracted the data from GitHub, we’ll use pandas to structure it into a DataFrame, making it easier to analyze and export, for example, as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051bef6-49e6-44df-b342-94ec2f6c05b6",
   "metadata": {},
   "source": [
    "These tools are chosen because they are widely used in the Python ecosystem for web scraping and data analysis. They offer the right balance of simplicity and power, making them ideal for this type of project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6e3f2-53a2-4fb8-a19c-55babde90a80",
   "metadata": {},
   "source": [
    "#### Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2da7ad-181f-4102-b61c-8be32d33cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4 --upgrade --quiet # installing BeautifulSoup modeule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f4e44a-5709-48fd-809e-caec5bd982e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary modules we will use in out project\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f15431-b3a2-40e6-8b83-989357d12f04",
   "metadata": {},
   "source": [
    "#### The URLs which we will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475cdf98-cf50-4b34-8d67-43dc14bdfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_topics_url = \"https://github.com/topics\"  # github topics page URL\n",
    "git_base_url = \"https://github.com\" # github official base URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e6818-0be4-4281-a670-275f5b98fd82",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from Github\n",
    "\n",
    "To scrape the list of topics from a GitHub page, we'll:\n",
    "\n",
    "* Use the requests library to download the page.\n",
    "* Use BeautifulSoup (BS4) to parse and extract the information.\n",
    "* Convert the extracted information to a Pandas DataFrame for easier manipulation.\n",
    "  \n",
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38ac006-20bc-4493-98da-9b37bdb16c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_page(topic_url):\n",
    "    # Dowload the web-page info\n",
    "    response = requests.get(topic_url)\n",
    "    # check for any Exception and catch it\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Load page.')\n",
    "        \n",
    "    # Use soup to parse page html info and return soup\n",
    "    topic_soup = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725cb53-0e0b-40e3-b1fc-81626d9f930f",
   "metadata": {},
   "source": [
    "The get_url_page function downloads the HTML content of a specified web page using its URL and parses it using BeautifulSoup. If the page fails to load, the function raises an exception. The result is a BeautifulSoup object that represents the parsed HTML, which can be used for further data extraction and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cf8462-4c90-4fa5-b97b-39ccfbceb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(soup):\n",
    "    \"\"\"\n",
    "    Extracts the titles of topics from the parsed HTML content.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of topic titles as strings.\n",
    "    \"\"\"\n",
    "    # CSS class for topic titles\n",
    "    topics_selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    # Find all <p> tags with the specified class\n",
    "    all_topics = soup.find_all('p', {'class': topics_selection_class})\n",
    "    \n",
    "    topics = []\n",
    "    \n",
    "    # Extract and clean text from each <p> tag\n",
    "    for topic in all_topics:\n",
    "        topics.append(topic.getText().strip())\n",
    "    return topics\n",
    "\n",
    "def get_topic_desc(soup):\n",
    "    \"\"\"\n",
    "    Extracts the descriptions of topics from the parsed HTML content.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of topic descriptions as strings.\n",
    "    \"\"\"\n",
    "    # CSS class for topic descriptions\n",
    "    topic_desc_class = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "    # Find all <p> tags with the specified class\n",
    "    all_topic_desc = soup.find_all('p', class_=topic_desc_class)\n",
    "\n",
    "    topic_desc = []\n",
    "\n",
    "    # Extract and clean text from each <p> tag\n",
    "    for desc in all_topic_desc:\n",
    "        topic_desc.append(desc.getText().strip())\n",
    "    return topic_desc\n",
    "\n",
    "def get_topic_url(soup):\n",
    "    \"\"\"\n",
    "    Extracts the URLs of topics from the parsed HTML content.\n",
    "\n",
    "    Parameters:\n",
    "    soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of topic URLs as strings.\n",
    "    \"\"\"\n",
    "    # Base URL for GitHub topics\n",
    "    git_base_url = 'https://github.com'\n",
    "    \n",
    "    # CSS class for topic URLs\n",
    "    all_url_selector_class = \"no-underline flex-grow-0\"\n",
    "    # Find all <a> tags with the specified class\n",
    "    all_anchor_tags = soup.find_all('a', class_=all_url_selector_class)\n",
    "    \n",
    "    all_url = []\n",
    "    \n",
    "    # Construct full URLs from relative paths\n",
    "    for tag in all_anchor_tags:\n",
    "        all_url.append(git_base_url + tag.get('href'))\n",
    "    return all_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec6ebe07-68c3-4cce-ae1a-9e9af5699be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_page(url):\n",
    "    \"\"\"\n",
    "    Scrapes the topics page and returns a DataFrame with topic titles, URLs, and descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the topics page to scrape.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the topic titles, URLs, and descriptions.\n",
    "    \"\"\"\n",
    "    # Get the BeautifulSoup object for the topics page\n",
    "    topic_soup = get_url_page(url)\n",
    "    \n",
    "    # Create a dictionary with the scraped data\n",
    "    my_dict = {\n",
    "        'Title': get_topic_titles(topic_soup),       # List of topic titles\n",
    "        'URLs': get_topic_url(topic_soup),           # List of topic URLs\n",
    "        'Description': get_topic_desc(topic_soup)    # List of topic descriptions\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a pandas DataFrame and return it\n",
    "    return pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842b8983-4821-45b2-b780-df75c28d4fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URLs</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "      <td>3D refers to the use of three-dimensional grap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                 URLs  \\\n",
       "0         3D         https://github.com/topics/3d   \n",
       "1       Ajax       https://github.com/topics/ajax   \n",
       "2  Algorithm  https://github.com/topics/algorithm   \n",
       "3        Amp      https://github.com/topics/amphp   \n",
       "4    Android    https://github.com/topics/android   \n",
       "\n",
       "                                         Description  \n",
       "0  3D refers to the use of three-dimensional grap...  \n",
       "1  Ajax is a technique for creating interactive w...  \n",
       "2  Algorithms are self-contained sequences that c...  \n",
       "3  Amp is a non-blocking concurrency library for ...  \n",
       "4  Android is an operating system built by Google...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = scrape_topics_page(github_topics_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe56aa1-ed12-4975-a2a0-378347b4202f",
   "metadata": {},
   "source": [
    "#### This is how the dataframe appears after the above operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66c9a7-9e47-412e-9bb1-b9b390842756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59097523-e9f4-47e7-be2b-33d373f2a487",
   "metadata": {},
   "source": [
    "## Getting info from the topics page URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900a46ff-5e59-4960-b26a-39f377640a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stars_count(span):\n",
    "    \"\"\"\n",
    "    Converts a star count string (e.g., '3.5k') to an integer.\n",
    "\n",
    "    Parameters:\n",
    "    span (Tag): The span tag containing the star count text.\n",
    "\n",
    "    Returns:\n",
    "    int: The star count as an integer.\n",
    "    \"\"\"\n",
    "    # Convert star count from 'k' format to an integer\n",
    "    span = float(span.getText().split('k')[0]) * 1000\n",
    "    return int(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "316aaf51-4bd4-4dfb-b186-ef3799e98454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tags, star_tags):\n",
    "    \"\"\"\n",
    "    Extracts repository information including the username, repository name, URL, and star count.\n",
    "\n",
    "    Parameters:\n",
    "    h3_tags (Tag): The h3 tag containing the repository's username and name.\n",
    "    star_tags (Tag): The span tag containing the star count for the repository.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the username, repository name, repository URL, and star count.\n",
    "    \"\"\"\n",
    "    # Extract all <a> tags within the h3 tag\n",
    "    all_a = h3_tags.find_all('a')\n",
    "    \n",
    "    # Extract and clean the username and repository name\n",
    "    username = all_a[0].text.strip()\n",
    "    repo = all_a[1].text.strip()\n",
    "    \n",
    "    # Construct the full repository URL\n",
    "    Repo_URL = git_base_url + all_a[1].get('href')\n",
    "    \n",
    "    # Get the star count for the repository\n",
    "    Stars = get_stars_count(star_tags)\n",
    "    \n",
    "    return username, repo, Repo_URL, Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aba58781-639c-43e5-970e-ccd453c4decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_soup):\n",
    "    \"\"\"\n",
    "    Extracts repository information for all repositories listed under a topic.\n",
    "\n",
    "    Parameters:\n",
    "    topic_soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content of the topic page.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the repository information, including username, repository name, URL, and star count.\n",
    "    \"\"\"\n",
    "    # CSS class for selecting <h3> tags that contain the repository names and usernames\n",
    "    h3_selector = \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    # Find all <h3> tags with the specified class\n",
    "    all_h3 = topic_soup.find_all('h3', class_=h3_selector)\n",
    "    \n",
    "    # Find all <span> tags that contain the star count\n",
    "    all_star_spans = topic_soup.find_all('span', id=\"repo-stars-counter-star\")\n",
    "    \n",
    "    # Initialize a dictionary to store repository information\n",
    "    repo_info = {\n",
    "        'username': [],\n",
    "        'reponame': [],\n",
    "        'repoURL': [],\n",
    "        'no_of_stars': []\n",
    "    }\n",
    "    \n",
    "    # Loop through each repository and extract information\n",
    "    for i in range(len(all_star_spans)):\n",
    "        repo_info['username'].append(get_repo_info(all_h3[i], all_star_spans[i])[0])\n",
    "        repo_info['reponame'].append(get_repo_info(all_h3[i], all_star_spans[i])[1])\n",
    "        repo_info['repoURL'].append(get_repo_info(all_h3[i], all_star_spans[i])[2])\n",
    "        repo_info['no_of_stars'].append(get_repo_info(all_h3[i], all_star_spans[i])[3])\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame and return it\n",
    "    return pd.DataFrame(repo_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92092b4c-a2e6-4dd3-bd94-5b729428d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics(topic_url, path):\n",
    "    \"\"\"\n",
    "    Scrapes repository information for a topic and saves it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    topic_url (str): The URL of the topic page to scrape.\n",
    "    path (str): The file path where the CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The {path} file already exists. Skipping.... \")\n",
    "        return\n",
    "    \n",
    "    # Get the repository information and save it to a CSV file\n",
    "    topic_df = get_topic_repos(get_url_page(topic_url))\n",
    "    topic_df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5d77098-475f-4c12-b7f0-fc353d5124df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>reponame</th>\n",
       "      <th>repoURL</th>\n",
       "      <th>no_of_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bregman-arie</td>\n",
       "      <td>devops-exercises</td>\n",
       "      <td>https://github.com/bregman-arie/devops-exercises</td>\n",
       "      <td>65800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>generative-ai-for-beginners</td>\n",
       "      <td>https://github.com/microsoft/generative-ai-for...</td>\n",
       "      <td>61700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pulumi</td>\n",
       "      <td>pulumi</td>\n",
       "      <td>https://github.com/pulumi/pulumi</td>\n",
       "      <td>20900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommenders-team</td>\n",
       "      <td>recommenders</td>\n",
       "      <td>https://github.com/recommenders-team/recommenders</td>\n",
       "      <td>18800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>danny-avila</td>\n",
       "      <td>LibreChat</td>\n",
       "      <td>https://github.com/danny-avila/LibreChat</td>\n",
       "      <td>17300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            username                     reponame  \\\n",
       "0       bregman-arie             devops-exercises   \n",
       "1          microsoft  generative-ai-for-beginners   \n",
       "2             pulumi                       pulumi   \n",
       "3  recommenders-team                 recommenders   \n",
       "4        danny-avila                    LibreChat   \n",
       "\n",
       "                                             repoURL  no_of_stars  \n",
       "0   https://github.com/bregman-arie/devops-exercises        65800  \n",
       "1  https://github.com/microsoft/generative-ai-for...        61700  \n",
       "2                   https://github.com/pulumi/pulumi        20900  \n",
       "3  https://github.com/recommenders-team/recommenders        18800  \n",
       "4           https://github.com/danny-avila/LibreChat        17300  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = get_url_page(df['URLs'][12])\n",
    "get_topic_repos(soup).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfea72a3-eafd-4c4f-84d9-3ed32ac21e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           https://github.com/topics/3d\n",
       "1         https://github.com/topics/ajax\n",
       "2    https://github.com/topics/algorithm\n",
       "3        https://github.com/topics/amphp\n",
       "4      https://github.com/topics/android\n",
       "Name: URLs, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['URLs'].head() # Checking for the five URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02b06645-32b4-417c-accf-25f8a2a1502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Angular file already exists. Skipping.... \n"
     ]
    }
   ],
   "source": [
    "scrape_topics(df['URLs'][5],df['Title'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d84b208-3bb6-47ea-b8ad-6de839c53e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repos(url):\n",
    "    \"\"\"\n",
    "    Scrapes the list of topics from the provided GitHub topics URL and their respective top repositories,\n",
    "    saving the data to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the GitHub topics page to scrape.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Inform the user that the scraping process has started\n",
    "    print(\"Scraping list of topics from GitHub topics:\")\n",
    "\n",
    "    # Scrape the list of topics and store them in a DataFrame\n",
    "    topics_df = scrape_topics_page(url)\n",
    "\n",
    "    # Create a directory named 'Data' to store the CSV files, if it doesn't exist\n",
    "    os.makedirs('Data', exist_ok=True)\n",
    "\n",
    "    # Iterate through each row in the DataFrame (each topic)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        # Inform the user about the topic currently being scraped\n",
    "        print(f\"Scraping top repositories for '{row['Title']}' \")\n",
    "\n",
    "        # Scrape the repositories for the current topic and save them to a CSV file\n",
    "        scrape_topics(row['URLs'], f\"Data/{row['Title']}.csv\" )\n",
    "\n",
    "    # Inform the user that the scraping process is complete\n",
    "    print(\"Done Scraping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b87ba-1779-4072-aa8f-411c616e136f",
   "metadata": {},
   "source": [
    "## The above function is the main function used for whole purpose, which does the following:\n",
    "* Starting Message: Notifies the user that the scraping process has started.\n",
    "* Scraping Topics: Explains that the topics are being scraped and stored in a DataFrame.\n",
    "* Creating Directory: Describes the creation of the Data directory to store CSV files.\n",
    "* Iterating Through Topics: Clarifies the iteration process over each topic to scrape repositories.\n",
    "* Completion Message: Indicates that the scraping process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6f93588-2f1f-4e78-9b7d-52b89566d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics from GitHub topics:\n",
      "Scraping top repositories for '3D' \n",
      "The Data/3D.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Ajax' \n",
      "The Data/Ajax.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Algorithm' \n",
      "The Data/Algorithm.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Amp' \n",
      "The Data/Amp.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Android' \n",
      "The Data/Android.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Angular' \n",
      "The Data/Angular.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Ansible' \n",
      "The Data/Ansible.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'API' \n",
      "The Data/API.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Arduino' \n",
      "The Data/Arduino.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'ASP.NET' \n",
      "The Data/ASP.NET.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Awesome Lists' \n",
      "The Data/Awesome Lists.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Amazon Web Services' \n",
      "The Data/Amazon Web Services.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Azure' \n",
      "The Data/Azure.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Babel' \n",
      "The Data/Babel.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Bash' \n",
      "The Data/Bash.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Bitcoin' \n",
      "The Data/Bitcoin.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Bootstrap' \n",
      "The Data/Bootstrap.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Bot' \n",
      "The Data/Bot.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'C' \n",
      "The Data/C.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Chrome' \n",
      "The Data/Chrome.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Chrome extension' \n",
      "The Data/Chrome extension.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Command-line interface' \n",
      "The Data/Command-line interface.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Clojure' \n",
      "The Data/Clojure.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Code quality' \n",
      "The Data/Code quality.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Code review' \n",
      "The Data/Code review.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Compiler' \n",
      "The Data/Compiler.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Continuous integration' \n",
      "The Data/Continuous integration.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'C++' \n",
      "The Data/C++.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Cryptocurrency' \n",
      "The Data/Cryptocurrency.csv file already exists. Skipping.... \n",
      "Scraping top repositories for 'Crystal' \n",
      "The Data/Crystal.csv file already exists. Skipping.... \n",
      "Done Scraping.\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos(github_topics_url) # An example of the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2c779-8615-49b7-bed6-a7379ca48b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
